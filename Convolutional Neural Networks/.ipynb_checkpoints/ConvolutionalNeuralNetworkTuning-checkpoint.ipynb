{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Read through the code in the following sections:\n",
    "    * [MNIST Data](#MNIST-Data)\n",
    "    * [Convolutional Neural Network Model](#Convolutional-Neural-Network-Model)\n",
    "    * [Train Model](#Train-Model)\n",
    "    * [Validation](#Validation)\n",
    "0. Complete the [Exercise](#Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow matplotlib graphics to display in the notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot\n",
    "\n",
    "# import numpy, for image dimension manipulation\n",
    "import numpy\n",
    "\n",
    "# import validation methods from scikit-learn\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# import the dataset and neural network layers from keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, ReLU, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "[MNIST](https://en.wikipedia.org/wiki/MNIST_database) is a famous dataset of images of handwritten numbers. The goal is to be able to figure out which number is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants to describe the MNIST images\n",
    "NUM_ROWS = 28 \n",
    "NUM_COLUMNS = 28\n",
    "NUM_COLORS = 1\n",
    "IMG_SHAPE = (NUM_ROWS, NUM_COLUMNS, NUM_COLORS)\n",
    "\n",
    "# constant to describe the MNIST output labels\n",
    "# there are ten different numbers, 0-9\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "((images_train, labels_train), (images_test, labels_test)) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one particular image and its label, to better understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14edf44c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.imshow(images_train[17], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_46 (Conv2D)           (None, 14, 14, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 7, 7, 8)           584       \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 8)                 3144      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                90        \n",
      "=================================================================\n",
      "Total params: 3,898\n",
      "Trainable params: 3,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Original provided example model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=8, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_53 (Conv2D)           (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "re_lu_42 (ReLU)              (None, 28, 28, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 14, 14, 8)         584       \n",
      "_________________________________________________________________\n",
      "re_lu_43 (ReLU)              (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 64)                25152     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 26,466\n",
      "Trainable params: 26,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# First convolutional model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=8, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(ReLU())\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(filters=8, kernel_size=3, strides=1, padding='same'))\n",
    "model.add(ReLU())\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_48 (Conv2D)           (None, 14, 14, 4)         40        \n",
      "_________________________________________________________________\n",
      "re_lu_37 (ReLU)              (None, 14, 14, 4)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 3, 3, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 3,058\n",
      "Trainable params: 3,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Second one-layer convolutional model with fewer parameters than the original\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(input_shape=IMG_SHAPE,\n",
    "                 filters=4, kernel_size=3, strides=2, padding='same'))\n",
    "model.add(ReLU())\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "\n",
    "# dense layers to consolidate information\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation='tanh'))\n",
    "\n",
    "# output layer to make the final decision on which number it is\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 1.3503\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.4118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x151180940>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_train = numpy.expand_dims(images_train, axis=3)\n",
    "\n",
    "# the labels need to be one-hot encoded, to match the ten outputs of our model\n",
    "labels_onehot_train = to_categorical(labels_train)\n",
    "\n",
    "# set up the model to be ready for training\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy')\n",
    "\n",
    "# fit the model to the training data\n",
    "model.fit(images_3d_train, labels_onehot_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras requires a color dimension, so we need to expand each image to have one\n",
    "images_3d_test = numpy.expand_dims(images_test, axis=3)\n",
    "\n",
    "# get the predictions from the model\n",
    "predictions_test_onehot = model.predict(images_3d_test)\n",
    "\n",
    "# get the index that has the highest probability\n",
    "predictions_test = numpy.argmax(predictions_test_onehot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9067"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the overall accuracy\n",
    "accuracy_score(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.9757384 , 0.98559856, 0.92407592, 0.89052438, 0.95482546,\n",
       "        0.82999045, 0.95268817, 0.95924765, 0.9144385 , 0.9029703 ]),\n",
       " array([0.94387755, 0.96475771, 0.89631783, 0.95841584, 0.94704684,\n",
       "        0.97421525, 0.92484342, 0.89299611, 0.87782341, 0.90386521]),\n",
       " array([0.95954357, 0.97506679, 0.90998524, 0.92322365, 0.95092025,\n",
       "        0.89633832, 0.93855932, 0.92493703, 0.89575694, 0.90341753]),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get precision, recall, f-score, and number of examples of each digit\n",
    "# can you see which digits are easiest for the model, and which are hardest?\n",
    "precision_recall_fscore_support(y_true=labels_test, y_pred=predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Can you get the model to perform better? Try adding more layers, or taking them away. \n",
    "Take a look at the documentation for the [convolutional](https://keras.io/layers/convolutional/) and [dense](https://keras.io/layers/core/) layers and the [sequential model](https://keras.io/models/sequential/) to understand the options that you have and try out different things.\n",
    "\n",
    "It might also be a good idea to find examples posted online of networks that did well with MNIST and try out some of the configuration they used. Make sure you cite any sources you use!\n",
    "\n",
    "Take notes of what performance you get from different configurations.\n",
    "\n",
    "Comment on what patterns you observed in terms of what changes helped your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications\n",
    "\n",
    "#### Model 1\n",
    "\n",
    "I completely reworked the architecture of the given convolutional neural network as follows:\n",
    "- Add **ReLU activation functions** after the two convolutional layers\n",
    "  - This adds a non-linearity to the convolutional layers and filters out noise to keep only the *positive* filter-image combinations\n",
    "- Decrease the **stride** of each convolutional layer from 2 to 1\n",
    "- Add a two-dimensional **max-pooling** layer (2x2 by default) after each ReLU activation in the convolutional part of the network\n",
    "  - Decreases the size of the input to the second layer while emphasizing *maximum* first-layer output (which is what we are aiming for—higher activations of a given filter should be propagated over \"adjacent noise\")\n",
    "- Increase the number of units in the **hidden layer** of the feed-forward section from 8 to 32.\n",
    "\n",
    "These changes allowed me to achieve 92.29% accuracy on the MNIST dataset—further details regarding F1 score, precision, and recall can be found above.\n",
    "\n",
    "#### Model 2\n",
    "\n",
    "I also built a model with a similar complexity (in terms of number of trainable parameters) to the originally provided example—decreasing training time by approximately 30%, reducing the number of trainable parameters from 3898 to 3058, and improving accuracy by approximately 25% (from ~49% to ~75%). Changes were as follows:\n",
    "\n",
    "- Add **ReLU activation functions** after the single convolutional layer\n",
    "  - This adds a non-linearity to the convolutional layers and filters out noise to keep only the *positive* filter-image combinations\n",
    "- Add a two-dimensional **max-pooling** layer (4x4) after the ReLU activation in the convolutional part of the network\n",
    "  - Decreases the size of the input to the second layer while emphasizing *maximum* first-layer output (which is what we are aiming for—higher activations of a given filter should be propagated over \"adjacent noise\")\n",
    "- Increase the number of units in the **hidden layer** of the feed-forward section from 8 to 64.\n",
    "- Decrease the **number of filters** in the convolutional layer from 8 to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
