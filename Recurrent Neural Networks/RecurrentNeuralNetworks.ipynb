{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Weights in RNNs\n",
    "\n",
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Look at the code in [Part A: Single Unit Simple Recurrent Layer](#Part-A:-Single-Unit-Simple-Recurrent-Layer) and complete the [Part A Exercise](#Part-A-Exercise)\n",
    "0. Look at the code in [Part B: Two Unit Simple Recurrent Layer](#Part-B:-Two-Unit-Simple-Recurrent-Layer) and complete the [Part B Exercise](#Part-B-Exercise)\n",
    "0. Optionally, look at the code in [Part C: LSTM Layer](#Part-C:-LSTM-Layer) and complete the [Part C Exercise](#Part-C-Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/Sources\n",
    "* [Class Notes](https://jennselby.github.io/MachineLearningCourseNotes/#recurrent-neural-networks)\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation\n",
    "* [Keras recurrent tutorial](https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Single Unit Simple Recurrent Layer\n",
    "\n",
    "Before we dive into something as complicated as LSTMs, Let's take a deeper look at simple recurrent layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neurons in the recurrent layer pass their output to the next layer, but also back to themselves. The input shape says that we'll be passing in one-dimensional inputs of unspecified length (the None is what makes it unspecified)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_unit_SRNN = Sequential()\n",
    "one_unit_SRNN.add(SimpleRNN(units=1, input_shape=(None, 1), activation='linear', use_bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[-0.55325055]], dtype=float32), array([[-1.]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "one_unit_SRNN_weights = one_unit_SRNN.get_weights()\n",
    "one_unit_SRNN_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the weights to whatever we want, to test out what happens with different weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[1.]], dtype=float32), array([[2.]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "one_unit_SRNN_weights[0][0][0] = 1\n",
    "one_unit_SRNN_weights[1][0][0] = 2\n",
    "one_unit_SRNN.set_weights(one_unit_SRNN_weights)\n",
    "one_unit_SRNN.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then pass in different input values, to see what the model outputs.\n",
    "\n",
    "The code below passes in a single sample that has three time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[25.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "one_unit_SRNN.predict(numpy.array([ [[3], [3], [7]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Exercise\n",
    "Figure out what the two weights in the one_unit_SRNN model control. Be sure to test your hypothesis thoroughly. Use different weights and different inputs."
   ]
  },
  {
   "source": [
    "Let the weight at [0,0,0] be $w_0$ and the weight at [1,0,0] be $w_1$. Values in parentheses in the diagram below show the intermediate output before summation and after multiplying by the corresponding weight.\n",
    "\n",
    "$w_0$ controls the effect of the *input* on the current timestep's recurrent layer. For example, suppose $w_0=2$ (and $w_1=1$). Then, the network computes the output value as follows:\n",
    "\n",
    "```\n",
    "      (6)\n",
    "[3] — w_0 → 6\n",
    "            |\n",
    "           w_1 (6)\n",
    "      (6)   ↓\n",
    "[3] — w_0 → 12\n",
    "            |\n",
    "           w_1 (12)\n",
    "     (14)   ↓\n",
    "[7] — w_0 → 26 → [26]\n",
    "```\n",
    "\n",
    "$w_1$ controls the value of the *recurrent layer from the previous timestep*'s effect on the current timestep. For example, suppose $w_0=1$ (and $w_1=2$). Then, the network computes the output value as follows:\n",
    "\n",
    "```\n",
    "      (3)\n",
    "[3] — w_0 → 3\n",
    "            |\n",
    "           w_1 (6)\n",
    "      (3)   ↓\n",
    "[3] — w_0 → 9\n",
    "            |\n",
    "           w_1 (18)\n",
    "      (7)   ↓\n",
    "[7] — w_0 → 25 → [25]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Two Unit Simple Recurrent Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_unit_SRNN = Sequential()\n",
    "two_unit_SRNN.add(SimpleRNN(units=2, input_shape=(None, 1), activation='linear', use_bias=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[-0.5520738 ,  0.15473759]], dtype=float32),\n",
       " array([[ 0.98677623,  0.1620891 ],\n",
       "        [ 0.1620891 , -0.9867761 ]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "two_unit_SRNN_weights = two_unit_SRNN.get_weights()\n",
    "two_unit_SRNN_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[1., 1.]], dtype=float32),\n",
       " array([[0., 1.],\n",
       "        [0., 1.]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "two_unit_SRNN_weights[0][0][0] = 1\n",
    "two_unit_SRNN_weights[0][0][1] = 1\n",
    "two_unit_SRNN_weights[1][0][0] = 0\n",
    "two_unit_SRNN_weights[1][0][1] = 1\n",
    "two_unit_SRNN_weights[1][1][0] = 0\n",
    "two_unit_SRNN_weights[1][1][1] = 1\n",
    "two_unit_SRNN.set_weights(two_unit_SRNN_weights)\n",
    "two_unit_SRNN.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This passes in a single sample with four time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 5., 31.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "two_unit_SRNN.predict(numpy.array([ [[3], [3], [7], [5]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B Exercise\n",
    "What do each of the six weights of the two_unit_SRNN control? Again, test out your hypotheses carefully."
   ]
  },
  {
   "source": [
    "The neural network above can be described as:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    r_0(t) \\\\\n",
    "    r_1(t)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    w_{1,0,0} & w_{1,1,0} \\\\\n",
    "    w_{1,0,1} & w_{1,1,1} \n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "    r_0(t-1) \\\\\n",
    "    r_1(t-1)\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "    w_{0,0,0} \\\\\n",
    "    w_{0,0,1}\n",
    "\\end{pmatrix}x(t)\n",
    "$$\n",
    "\n",
    "where $r_i(t)$ represents the value of the $i$-th node of the recurrent layer at time $t$, and $x(t)$ represents the input to the network at time $t$. Applied to the input data, this yields (for $t=0,1,2,3$):\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    r_0(0) \\\\\n",
    "    r_1(0)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    0 & 0 \\\\\n",
    "    1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "    0 \\\\\n",
    "    0\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{pmatrix}\\cdot 3 = \\begin{pmatrix}\n",
    "    3 \\\\\n",
    "    3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    r_0(1) \\\\\n",
    "    r_1(1)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    0 & 0 \\\\\n",
    "    1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "    3 \\\\\n",
    "    3\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{pmatrix}\\cdot 3 = \\begin{pmatrix}\n",
    "    3 \\\\\n",
    "    9\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    r_0(2) \\\\\n",
    "    r_1(2)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    0 & 0 \\\\\n",
    "    1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "    3 \\\\\n",
    "    9\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{pmatrix}\\cdot 7 = \\begin{pmatrix}\n",
    "    7 \\\\\n",
    "    19\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    r_0(3) \\\\\n",
    "    r_1(3)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    0 & 0 \\\\\n",
    "    1 & 1\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "    7 \\\\\n",
    "    19\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "    1 \\\\\n",
    "    1\n",
    "\\end{pmatrix}\\cdot 5 = \\begin{pmatrix}\n",
    "    5 \\\\\n",
    "    31\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: LSTM Layer\n",
    "### Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_unit_LSTM = Sequential()\n",
    "one_unit_LSTM.add(LSTM(units=1, input_shape=(None, 1),\n",
    "                       activation='linear', recurrent_activation='linear',\n",
    "                       use_bias=False, unit_forget_bias=False,\n",
    "                       kernel_initializer='zeros',\n",
    "                       recurrent_initializer='zeros',\n",
    "                       return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "one_unit_LSTM_weights = one_unit_LSTM.get_weights()\n",
    "one_unit_LSTM_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[1., 0., 1., 1.]], dtype=float32),\n",
       " array([[0., 0., 0., 0.]], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "one_unit_LSTM_weights[0][0][0] = 1\n",
    "one_unit_LSTM_weights[0][0][1] = 0\n",
    "one_unit_LSTM_weights[0][0][2] = 1\n",
    "one_unit_LSTM_weights[0][0][3] = 1\n",
    "one_unit_LSTM_weights[1][0][0] = 0\n",
    "one_unit_LSTM_weights[1][0][1] = 0\n",
    "one_unit_LSTM_weights[1][0][2] = 0\n",
    "one_unit_LSTM_weights[1][0][3] = 0\n",
    "one_unit_LSTM.set_weights(one_unit_LSTM_weights)\n",
    "one_unit_LSTM.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[ 0.],\n",
       "        [ 1.],\n",
       "        [ 8.],\n",
       "        [64.]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "one_unit_LSTM.predict(numpy.array([ [[0], [1], [2], [4]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Exercise\n",
    "### Optional\n",
    "Conceptually, the [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) has several _gates_:\n",
    "\n",
    "* __Forget gate__: these weights allow some long-term memories to be forgotten.\n",
    "* __Input gate__: these weights decide what new information will be added to the context cell.\n",
    "* __Output gate__: these weights decide what pieces of the new information and updated context will be passed on to the output.\n",
    "\n",
    "It also has a __cell__ that can hold onto information from the current input (as well as things it has remembered from previous inputs), so that it can be used in later outputs.\n",
    "\n",
    "Identify which weights in the one_unit_LSTM model are connected with the context and which are associated with the three gates. This is considerably more difficult to do by looking at the inputs and outputs, so you could also treat this as a code reading exercise and look through the keras code to find the answer.\n",
    "\n",
    "_Note_: The output from the predict call is what the linked explanation calls $h_{t}$."
   ]
  },
  {
   "source": [
    "Weights of the form $w_{0,i,j}$ connect the input layer to the LSTM layer, while weights of the form $w_{1,i,j}$ connect the LSTM layer to itself.\n",
    "\n",
    "Weights of the form $w_{i,j,0}$ connect to the *input gate*.\n",
    "\n",
    "Weights of the form $w_{i,j,1}$ connect to the *forget gate*.\n",
    "\n",
    "Weights of the form $w_{i,j,2}$ connect to the *cell state*.\n",
    "\n",
    "Weights of the form $w_{i,j,3}$ connect to the *output gate*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python375jvsc74a57bd057baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6",
   "display_name": "Python 3.7.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}